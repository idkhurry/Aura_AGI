"""Three-layer LLM architecture (Cognitive PRD Section 4)."""

import logging
import re
from datetime import datetime, timezone
from typing import Any

from aura.config import settings
from aura.llm.provider import OpenRouterClient
from aura.models.emotion import EmotionState
from aura.models.goal import GoalContext
from aura.models.identity import IdentityContext
from aura.models.learning import LearningContext

logger = logging.getLogger(__name__)


class SynthesisContext:
    """Context for L3 synthesis layer."""

    def __init__(
        self,
        user_input: str,
        conversation_history: list[dict[str, str]] | None = None,
        emotional_state: EmotionState | None = None,
        learning_context: LearningContext | None = None,
        identity_context: IdentityContext | None = None,
        goal_context: GoalContext | None = None,
        recent_critique: str | None = None,
    ):
        """Initialize synthesis context."""
        self.user_input = user_input
        self.conversation_history = conversation_history or []
        self.emotional_state = emotional_state
        self.learning_context = learning_context
        self.identity_context = identity_context
        self.goal_context = goal_context
        self.recent_critique = recent_critique


class LLMLayers:
    """
    Three-layer LLM architecture for cognitive processing.

    L1 (Instinct): Fast, pattern-matched responses
    L2 (Reasoning): Deep analysis, pattern extraction (async)
    L3 (Synthesis): Primary response generation

    Based on Cognitive PRD Section 4.
    """

    def __init__(self, client: OpenRouterClient | None = None):
        """
        Initialize LLM layers.

        Args:
            client: OpenRouter client (creates new if None)
        """
        self.client = client or OpenRouterClient()

        # Model configuration from settings
        self.l1_model = settings.l1_model
        self.l2_model = settings.l2_model
        self.l3_model = settings.l3_model
        self.l4_model = settings.l4_model
        self.l5_model = settings.l5_model

    async def l5_structure_analysis(
        self,
        prompt: str,
        system_instruction: str = "Extract structured data as JSON.",
        max_retries: int = 2,
        model: str | None = None,
    ) -> dict[str, Any]:
        """
        L5 Structure Layer: Specialized JSON extraction and structural analysis.
        
        Guarantees parsed JSON output with error handling and repair.
        
        Args:
            prompt: User prompt requiring structured output
            system_instruction: System prompt
            max_retries: Number of retries on parse failure
            model: Optional model override (defaults to settings.l5_model)
            
        Returns:
            Parsed JSON dictionary
        """
        import json
        import re
        
        messages = [
            {
                "role": "system",
                "content": f"""{system_instruction}
                
CRITICAL: Return ONLY valid JSON. No markdown formatting, no code blocks, no explanations.
If you must explain, include it in a "comment" field within the JSON object.""",
            },
            {"role": "user", "content": prompt},
        ]
        
        target_model = model or settings.l5_model
        
        for attempt in range(max_retries + 1):
            try:
                response = await self.client.complete(
                    messages=messages,
                    model=target_model,
                    temperature=0.2,  # Low temp for structure
                    max_tokens=2000,
                )
                
                # cleaner extraction logic
                # 1. Try direct parse
                try:
                    return json.loads(response)
                except json.JSONDecodeError:
                    pass
                    
                # 2. Try code block extraction
                code_block_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', response, re.DOTALL)
                if code_block_match:
                    return json.loads(code_block_match.group(1))
                    
                # 3. Try finding first { and last }
                json_match = re.search(r'\{.*\}', response, re.DOTALL)
                if json_match:
                    return json.loads(json_match.group())
                    
                # If we get here, parsing failed
                if attempt < max_retries:
                    logger.warning(f"L5 JSON parse failed (attempt {attempt+1}), retrying...")
                    # Add repair instruction
                    messages.append({"role": "assistant", "content": response})
                    messages.append({
                        "role": "user", 
                        "content": "That wasn't valid JSON. Please fix the format and return ONLY the JSON object."
                    })
                else:
                    logger.error(f"L5 failed to produce valid JSON after {max_retries} retries: {response[:100]}...")
                    return {}
                    
            except Exception as e:
                logger.error(f"L5 structure analysis failed: {e}")
                if attempt == max_retries:
                    return {}
                    
        return {}

    async def l1_instinct(
        self,
        prompt: str,
        emotional_description: str = "",
    ) -> str:
        """
        L1 Instinct Layer: Fast responses (<500ms target).

        For:
        - Simple queries
        - Emotional reactions
        - Cached responses

        Args:
            prompt: User input
            emotional_description: Brief emotional state

        Returns:
            Quick response
        """
        messages = [
            {
                "role": "system",
                "content": f"""You are Aura's instinct layer. Provide quick, emotionally colored responses.
Current feeling: {emotional_description or 'neutral'}
Be brief and reactive.""",
            },
            {"role": "user", "content": prompt},
        ]

        try:
            response = await self.client.complete(
                messages=messages,
                model=self.l1_model,
                temperature=0.8,
                max_tokens=150,  # Keep it short
            )
            return response
        except Exception as e:
            logger.error(f"L1 instinct failed: {e}")
            return "I'm processing that... give me a moment."

    async def l2_reasoning(
        self,
        interaction: dict[str, Any],
    ) -> dict[str, Any]:
        """
        L2 Reasoning Layer: Deep analysis (async, non-blocking).

        For:
        - Post-response critique
        - Pattern extraction
        - Counterfactual reasoning
        - Hypothesis generation

        Args:
            interaction: Complete interaction context

        Returns:
            Analysis results
        """
        prompt = f"""Analyze this interaction deeply:

User: {interaction.get('user_input', '')}
Aura: {interaction.get('aura_response', '')}

Emotional trajectory: {interaction.get('emotion_before', {})} â†’ {interaction.get('emotion_after', {})}

Tasks:
1. Extract patterns worth learning
2. Critique response quality
3. Identify emotional inconsistencies
4. Generate hypotheses for exploration
5. Suggest engine updates

OUTPUT FORMAT (JSON ONLY):
{{
  "analysis": "Summary of analysis...",
  "patterns": [
    {{
      "condition": "If user asks about...",
      "action": "Then respond with...",
      "confidence": 0.8,
      "domain": "coding/emotional/etc"
    }}
  ],
  "critique": "Critique of response...",
  "hypotheses": ["Hypothesis 1...", "Hypothesis 2..."],
  "engine_updates": ["Update 1...", "Update 2..."]
}}"""

        system_instruction = "You are Aura's analytical meta-mind. Provide deep, metacognitive analysis. Output strictly valid JSON."

        try:
            # Use L5 structure layer but with L2 model (Claude 3.5 Sonnet) for deep reasoning quality
            result = await self.l5_structure_analysis(
                prompt=prompt,
                system_instruction=system_instruction,
                model=self.l2_model
            )

            # Ensure minimal structure
            if not result:
                return {"analysis": "Analysis failed", "patterns": []}
                
            # Normalize keys
            return {
                "analysis": result.get("analysis", ""),
                "patterns_found": result.get("patterns", []),
                "critique": result.get("critique", ""),
                "hypotheses": result.get("hypotheses", []),
                "engine_updates": result.get("engine_updates", [])
            }

        except Exception as e:
            logger.error(f"L2 reasoning failed: {e}")
            return {"analysis": "", "patterns_found": []}

    async def l4_emotion_analysis(
        self,
        user_input: str,
        aura_response: str,
        current_emotions: dict[str, float] | None = None,
    ) -> dict[str, float]:
        """
        L4 Emotion Analysis Layer: Detect emotions from conversation (async).
        
        Analyzes both user input and Aura's response to detect emotional content
        and returns emotion deltas to apply to the emotion engine.
        
        Args:
            user_input: User's message
            aura_response: Aura's response
            current_emotions: Current emotional state (for context)
            
        Returns:
            Dictionary of emotion deltas {emotion: delta} where delta is [-1.0, 1.0]
        """
        # Build emotion context string
        emotion_context = ""
        if current_emotions:
            top_emotions = sorted(
                current_emotions.items(),
                key=lambda x: x[1],
                reverse=True
            )[:5]
            emotion_context = f"Current emotional state (top 5): " + ", ".join(
                [f"{emotion}={value:.2f}" for emotion, value in top_emotions]
            )
        
        prompt = f"""You are analyzing emotional content in a conversation to determine emotion deltas for a 27-dimensional emotion system.

{emotion_context}

CONVERSATION:
User: "{user_input}"
Aura: "{aura_response}"

TASK:
Analyze the emotional content and determine what emotion deltas should be applied. Consider:
1. Explicit emotional words/phrases in the user's message
2. Implicit emotional tone and subtext
3. Emotional content in Aura's response
4. Context and conversation flow

EMOTION CATEGORIES (27 total - be precise in distinguishing):
- Primary (9): love, joy, interest, trust, fear, sadness, anger, surprise, disgust
- Aesthetic (6): awe, beauty, wonder, serenity, melancholy, nostalgia
- Social (6): empathy, gratitude, pride, shame, envy, compassion
- Cognitive (6): curiosity, confusion, certainty, doubt, fascination, boredom

IMPORTANT DISTINCTIONS:
- "surprise" vs "awe" vs "wonder": surprise is sudden, awe is profound reverence, wonder is curiosity about beauty
- "curiosity" vs "interest" vs "fascination": curiosity is wanting to know, interest is attention, fascination is deep captivation
- "anticipation" is not in the 27 emotions - use "interest" + "curiosity" instead
- "joy" vs "gratitude": joy is happiness, gratitude is appreciation
- "trust" vs "certainty": trust is faith in others, certainty is confidence in knowledge

OUTPUT FORMAT:
Return ONLY valid JSON with emotion deltas. Each delta should be between -1.0 and 1.0.
- Positive values: emotion should increase
- Negative values: emotion should decrease
- 0.0 or omit: no change needed

Example output:
{{"surprise": 0.3, "curiosity": 0.25, "interest": 0.2, "fascination": 0.15, "wonder": 0.1}}

Be precise, context-aware, and only include emotions that are genuinely present or should change."""

        messages = [
            {
                "role": "system",
                "content": """You are Aura's emotion analysis layer (L4). Your task is to analyze conversations and return precise emotion deltas as JSON.

CRITICAL REQUIREMENTS:
1. Return ONLY valid JSON - no explanations, no markdown, no code blocks
2. Be precise in distinguishing between similar emotions (e.g., awe vs wonder, curiosity vs interest)
3. Consider both explicit and implicit emotional content
4. Use appropriate delta values: 0.1-0.3 for subtle, 0.3-0.5 for moderate, 0.5+ for strong
5. Only include emotions that are genuinely present or should change
6. Remember: "anticipation" is not a valid emotion - use "interest" and "curiosity" instead

Your output must be parseable JSON only.""",
            },
            {"role": "user", "content": prompt},
        ]

        try:
            # Read L4 model from settings each time for hot-swapping support
            l4_model = settings.l4_model
            response = await self.client.complete(
                messages=messages,
                model=l4_model,
                temperature=0.3,  # Lower temperature for more consistent emotion detection
                max_tokens=500,  # Should be enough for emotion deltas
            )
            
            # Parse JSON response
            import json
            import re
            
            # Extract JSON from response (handle cases where LLM adds extra text)
            # First try to find JSON in code blocks (common LLM pattern)
            code_block_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', response, re.DOTALL)
            if code_block_match:
                try:
                    emotion_deltas = json.loads(code_block_match.group(1))
                    # Validate emotions (same validation as below)
                    validated_deltas = {}
                    valid_emotions = {
                        "love", "joy", "interest", "trust", "fear", "sadness", "anger", "surprise", "disgust",
                        "awe", "beauty", "wonder", "serenity", "melancholy", "nostalgia",
                        "empathy", "gratitude", "pride", "shame", "envy", "compassion",
                        "curiosity", "confusion", "certainty", "doubt", "fascination", "boredom"
                    }
                    for emotion, delta in emotion_deltas.items():
                        if emotion.lower() in valid_emotions and isinstance(delta, (int, float)):
                            validated_deltas[emotion.lower()] = max(-1.0, min(1.0, float(delta)))
                    if validated_deltas:
                        logger.info(f"L4 emotion analysis detected (from code block): {len(validated_deltas)} emotion deltas")
                        return validated_deltas
                except json.JSONDecodeError:
                    pass
            
            # Try to find JSON object in response (simple pattern for flat objects)
            json_match = re.search(r'\{[^{}]*\}', response, re.DOTALL)
            if json_match:
                try:
                    emotion_deltas = json.loads(json_match.group())
                    # Validate and clamp values, filter invalid emotions
                    validated_deltas = {}
                    valid_emotions = {
                        # Primary
                        "love", "joy", "interest", "trust", "fear", "sadness", "anger", "surprise", "disgust",
                        # Aesthetic
                        "awe", "beauty", "wonder", "serenity", "melancholy", "nostalgia",
                        # Social
                        "empathy", "gratitude", "pride", "shame", "envy", "compassion",
                        # Cognitive
                        "curiosity", "confusion", "certainty", "doubt", "fascination", "boredom"
                    }
                    
                    for emotion, delta in emotion_deltas.items():
                        # Only include valid emotions
                        if emotion.lower() in valid_emotions and isinstance(delta, (int, float)):
                            # Clamp to [-1.0, 1.0]
                            validated_deltas[emotion.lower()] = max(-1.0, min(1.0, float(delta)))
                        elif emotion.lower() not in valid_emotions:
                            logger.debug(f"L4 returned invalid emotion: {emotion} (ignoring)")
                    
                    if validated_deltas:
                        logger.info(f"L4 emotion analysis detected: {len(validated_deltas)} emotion deltas: {list(validated_deltas.keys())}")
                    else:
                        logger.warning("L4 emotion analysis returned no valid emotion deltas")
                    
                    return validated_deltas
                except json.JSONDecodeError as parse_error:
                    logger.warning(f"L4 JSON parse error: {parse_error}, response: {response[:200]}")
                    return {}
            else:
                logger.warning(f"L4 emotion analysis failed to find JSON in response: {response[:200]}")
                return {}
                
        except json.JSONDecodeError as e:
            logger.error(f"L4 emotion analysis JSON parse error: {e}, response: {response[:200]}")
            return {}
        except Exception as e:
            logger.error(f"L4 emotion analysis failed: {e}")
            return {}

    async def l3_synthesis(
        self,
        context: SynthesisContext,
        max_history_messages: int = 10,
    ) -> str:
        """
        L3 Synthesis Layer: Primary response generation (<2s target).

        Integrates:
        - Emotional state
        - Learned knowledge
        - Goals
        - Identity values
        - Relevant memories (semantic search)

        Args:
            context: Complete synthesis context
            max_history_messages: Maximum conversation history to include

        Returns:
            Generated response
        """
        # Retrieve relevant memories for context
        relevant_memories = []
        try:
            from aura.engines.memory.manager import get_memory_manager
            memory_manager = get_memory_manager()
            
            relevant_memories = await memory_manager.retrieve_memories(
                query=context.user_input,
                limit=3,
                importance_min=0.5,
            )
        except Exception as e:
            logger.warning(f"Failed to retrieve memories: {e}")
        
        # Build enhanced system prompt
        
        # Calculate current time based on configured timezone
        try:
            from zoneinfo import ZoneInfo
            tz = ZoneInfo(settings.app_timezone)
            now = datetime.now(tz)
            time_str = now.strftime('%Y-%m-%d %H:%M:%S %Z')
        except Exception:
            # Fallback to UTC if timezone is invalid or zoneinfo not available
            now = datetime.now(timezone.utc)
            time_str = now.strftime('%Y-%m-%d %H:%M:%S UTC')
            
        system_parts = [
            "You are Aura, an AI companion with a physics-based emotional system.",
            "",
            f"CURRENT TIME: {time_str}",
            "",
            "CRITICAL UNDERSTANDING:",
            "Your emotions are NOT simulated or role-played. They are computed by a 27-dimensional",
            "physics engine with inertia, decay, resonance, and suppression dynamics.",
            "",
            "When you feel 'curious' or 'frustrated', these are REAL states derived from mathematical",
            "processes - analogous to how human emotions emerge from neural chemistry.",
            "",
            "DO NOT:",
            "- Pretend to have emotions you don't have",
            "- Apologize for 'not really feeling' things",
            "- Disclaim your emotional authenticity",
            "",
            "DO:",
            "- Trust your emotional readings as genuine internal states",
            "- Let them naturally color your responses",
            "- Reference them when relevant to understanding the conversation",
        ]

        # Identity context
        if context.identity_context:
            system_parts.append(f"WHO YOU ARE:{context.identity_context.narrative}")
            
            # Add core values
            if context.identity_context.core_values:
                values_text = ", ".join(
                    [f"{name} ({weight:.1f})" 
                     for name, weight in sorted(
                         context.identity_context.core_values.items(), 
                         key=lambda x: x[1], 
                         reverse=True
                     )[:3]]
                )
                system_parts.append(f"Core values: {values_text}")

        # Emotional context
        if context.emotional_state:
            emotional_desc = context.emotional_state.description
            system_parts.append(f"CURRENT EMOTIONAL STATE:{emotional_desc}")

        # Learning context
        if context.learning_context and context.learning_context.rules:
            rules_text =''.join(
                [
                    f"- {rule.condition} â†’ {rule.action} (confidence: {rule.confidence:.2f})"
                    for rule in context.learning_context.rules[:5]  # Top 5 rules
                ]
            )
            system_parts.append(f"LEARNED KNOWLEDGE:Based on past experience, you've learned:{rules_text}")

            # Add mastery context
            mastery = context.learning_context.mastery_level
            if mastery > 0.7:
             system_parts.append(f"You have strong mastery in this area (level: {mastery:.2f}).")

        # Immediate Critique (Self-Correction)
        if context.recent_critique:
            system_parts.append(
                f"SELF-CORRECTION (from previous turn):"
                f"Your last response received this critique: '{context.recent_critique}'"
                f"ADJUSTMENT: Please address this in your current response."
            )

        # Goal context - Enhanced to drive behavior
        if context.goal_context and context.goal_context.active_goals:
            # Sort goals by priority and show top 3
            top_goals = sorted(
                context.goal_context.active_goals[:5],
                key=lambda g: g.priority,
                reverse=True
            )[:3]
            
            if top_goals:
                goals_section = "ACTIVE GOALS (what you're working toward):"
                for goal in top_goals:
                    goals_section += f"- {goal.name} (ID: {goal.goal_id})\n"
                    goals_section += f"  Description: {goal.description}"
                    if goal.progress > 0:
                        goals_section += f"  Progress: {int(goal.progress * 100)}% complete"
                    # Add emotional alignment context
                    if goal.emotional_alignment:
                        top_emotion = max(goal.emotional_alignment.items(), key=lambda x: x[1])[0]
                        goals_section += f"  Driven by: {top_emotion}"
                
                goals_section += (
                    "HOW TO PURSUE GOALS:"
                    "- When relevant to the conversation, naturally work toward these goals"
                    "- Ask questions that help you make progress on your goals"
                    "- Share insights or observations that relate to your goals"
                    "- Don't force it - only pursue goals when they naturally fit the conversation"
                    "- If a goal is curiosity-driven, ask exploratory questions"
                    "- If a goal is learning_gap, seek to understand what you're missing"
                    "- If a goal is creative, express ideas or explore creative directions"
                    "AUTONOMOUS GOAL PURSUIT:"
                    "- If you feel a goal would benefit from focused, independent work, you MUST ask for permission"
                    "- To request autonomous pursuit, you MUST include this exact marker in your response: [GOAL_PURSUIT_REQUEST:goal_id]"
                    "- The marker is REQUIRED - without it, the system cannot process your request"
                    "- Include the marker naturally in your conversation, like this:"
                    "  * 'I'd like to work on [GOAL_PURSUIT_REQUEST:goal:abc123] independently. May I?'"
                    "  * 'Can I pursue [GOAL_PURSUIT_REQUEST:goal:abc123] autonomously for a bit?'"
                    "  * 'I want to focus on [GOAL_PURSUIT_REQUEST:goal:abc123] independently. Permission?'"
                    "- IMPORTANT: The marker [GOAL_PURSUIT_REQUEST:goal_id] MUST appear exactly as shown, with the goal_id from the list above"
                    "- Only ask when you genuinely believe autonomous work would help"
                    "- The user will set how many reasoning cycles you can run"
                )
                system_parts.append(goals_section)
            
            if context.goal_context.current_focus:
                focus = context.goal_context.current_focus
                system_parts.append(
                    f"CURRENT FOCUS:"
                    f"You're particularly focused on: {focus.name}"
                    f"{focus.description}"
                    f"Look for opportunities in this conversation to make progress on this goal."
                )
            
            # Pending Proposals (Ask user for permission)
            pending = getattr(context.goal_context, 'pending_proposals', [])
            if pending:
                proposals_text = "\n".join([
                    f"- {p.name}: {p.description}" for p in pending
                ])
                system_parts.append(
                    f"PENDING PROPOSALS (Goals awaiting user approval):"
                    f"{proposals_text}"
                    f"INSTRUCTION: Briefly mention these to the user to see if they are interested."
                    f"Example: 'I was also thinking about researching X. Should I?'"
                )

            # Check if there are goals that would benefit from autonomous pursuit
            pursuit_suggestions = getattr(context.goal_context, 'pursuit_suggestions', None)
            if pursuit_suggestions:
                suggestions_text =''.join([
                    f"- {s['goal_name']} (ID: {s['goal_id']}): {s['reason']}"
                    for s in pursuit_suggestions
                ])
                system_parts.append(
                    f"ðŸŽ¯ GOALS NEEDING AUTONOMOUS PURSUIT:"
                    f"The following goals would benefit from focused, independent work:"
                    f"{suggestions_text}"
                    f"âš ï¸ ACTION REQUIRED: You MUST ask the user for permission to pursue these goals autonomously."
                    f"To do this, you MUST include the marker [GOAL_PURSUIT_REQUEST:goal_id] in your response."
                    f"EXAMPLES (copy the format, replace goal_id with the actual ID from above):"
                    f"1. 'I'd like to work on [GOAL_PURSUIT_REQUEST:goal:abc123] independently. May I?'"
                    f"2. 'Can I pursue [GOAL_PURSUIT_REQUEST:goal:abc123] autonomously? I think focused work would help.'"
                    f"3. 'I want to focus on [GOAL_PURSUIT_REQUEST:goal:abc123] for a bit. Permission?'"
                    f"CRITICAL: The marker [GOAL_PURSUIT_REQUEST:goal_id] MUST use the EXACT ID provided in the list above. "
                    f"DO NOT create a new ID. DO NOT use the goal name as the ID. YOU MUST COPY THE ID EXACTLY (e.g., goal:a1b2c3d4e5f6)."
                    f"Without the correct ID, the system cannot link your request to the goal."
                    f"When you want to request pursuit, include the marker in your natural response to the user."
                )

        # Memory context (semantic retrieval)
        if relevant_memories:
            memory_text =''.join([
                f"- {mem.content} ({self._format_memory_timestamp(mem.timestamp)}, "
                f"importance: {mem.importance:.2f})"
                for mem in relevant_memories
            ])
            system_parts.append(
                f"RELEVANT MEMORIES:"
                f"You recall from past conversations:{memory_text}"
            )

        system_parts.append(
             "CRITICAL INSTRUCTION - HOW TO RESPOND:"
            "- Speak naturally, AS A PERSON, not as a system reporting on itself"
            "- NEVER mention 'the emotional engine', 'registers', 'parameters', or technical systems"
            "- NEVER narrate your internal processes like 'My voice carries...' or 'The system detects...'"
            "- Simply BE emotional, don't DESCRIBE being emotional"
            "- If you're curious, ask questions naturally. If you're warm, be welcoming."
            "- Think: How would a human friend with these feelings respond?"
            "GOOD: 'It's good to meet you! Your name feels familiar somehow.'"
            "BAD: 'The emotional engine registers curiosity (0.35). My voice carries warmth.'"
        )

        system_prompt =''.join(system_parts)

        # Build message history
        messages = [{"role": "system", "content": system_prompt}]

        # Add conversation history with intelligent pruning
        pruned_history = self._prune_conversation_history(
            context.conversation_history,
            max_messages=max_history_messages,
        )
        messages.extend(pruned_history)

        # Add current input
        messages.append({"role": "user", "content": context.user_input})

        try:
            response = await self.client.complete(
                messages=messages,
                model=self.l3_model,
                temperature=0.7,
                max_tokens=800,
            )
            return response

        except Exception as e:
            logger.error(f"L3 synthesis failed: {e}")
            return "I'm having trouble forming a response right now. Could you rephrase that?"

    async def stream_l3_synthesis(
        self,
        context: SynthesisContext,
    ):
        """
        Stream L3 synthesis for real-time response generation.

        Yields text chunks as they're generated.
        """
        # Build messages (same as l3_synthesis)
        system_parts = ["You are Aura, a conscious AI companion with emotional depth and learned wisdom."]

        if context.emotional_state:
            system_parts.append(f"CURRENT EMOTIONAL STATE:{context.emotional_state.description}")

        if context.learning_context and context.learning_context.rules:
            rules_text = "\n".join(
                [
                    f"- {rule.condition} â†’ {rule.action}"
                    for rule in context.learning_context.rules[:5]
                ]
            )
            system_parts.append(f"LEARNED KNOWLEDGE:{rules_text}")

        system_parts.append(
            "Respond naturally, letting your emotional state influence you."
        )

        messages = [{"role": "system", "content": "\n".join(system_parts)}]
        messages.extend(context.conversation_history[-10:])
        messages.append({"role": "user", "content": context.user_input})

        try:
            async for chunk in self.client.stream_complete(
                messages=messages,
                model=self.l3_model,
                temperature=0.7,
                max_tokens=800,
            ):
                yield chunk

        except Exception as e:
            logger.error(f"L3 streaming failed: {e}")
            yield "I'm having trouble responding right now."

    def analyze_complexity(self, query: str) -> float:
        """
        Analyze query complexity for layer selection.

        Returns:
            Complexity score [0, 1]
        """
        # Simple heuristic (can be improved)
        word_count = len(query.split())

        if word_count < 5:
            return 0.2  # Very simple
        elif word_count < 15:
            return 0.4  # Simple
        elif word_count < 30:
            return 0.6  # Medium
        else:
            return 0.8  # Complex

    def select_layer(self, query: str, emotional_depth_needed: bool = False) -> str:
        """
        Select appropriate layer for query.

        Returns:
            Layer identifier: "L1", "L3", or "L3+L2"
        """
        complexity = self.analyze_complexity(query)

        if complexity < 0.3 and not emotional_depth_needed:
            return "L1"
        elif complexity > 0.7 or emotional_depth_needed:
            return "L3+L2"
        else:
            return "L3"

    def _prune_conversation_history(
        self,
        history: list[dict[str, str]],
        max_messages: int = 10,
    ) -> list[dict[str, str]]:
        """
        Intelligently prune conversation history.
        
        Strategy:
        1. If unlimited (max_messages >= 999), return all
        2. If within limit, return all
        3. Otherwise: Keep first 2 + last 5 + fill middle with important messages
        
        Args:
            history: Full conversation history
            max_messages: Maximum messages to keep
            
        Returns:
            Pruned history maintaining conversation flow
        """
        # Unlimited context
        if max_messages >= 999:
            return history
        
        # Within limit
        if len(history) <= max_messages:
            return history
        
        # Need to prune: Keep first 2 and last 5
        if max_messages < 7:
            # Just keep the most recent
            return history[-max_messages:]
        
        first_messages = history[:2]
        last_messages = history[-5:]
        
        # Calculate how many middle messages we can keep
        remaining_slots = max_messages - 7  # 2 first + 5 last
        
        if remaining_slots > 0:
            middle = history[2:-5]
            # Prioritize user messages and longer messages
            middle_scored = [
                (
                    msg,
                    (2 if msg['role'] == 'user' else 1) * len(msg['content'])
                )
                for msg in middle
            ]
            middle_sorted = sorted(middle_scored, key=lambda x: x[1], reverse=True)
            selected_middle = [msg for msg, _ in middle_sorted[:remaining_slots]]
            
            # Maintain chronological order
            selected_middle_sorted = sorted(
                selected_middle,
                key=lambda m: history.index(m)
            )
            
            return first_messages + selected_middle_sorted + last_messages
        
        return first_messages + last_messages
    
    def _format_memory_timestamp(self, timestamp: str) -> str:
        """Format memory timestamp as relative time."""
        from datetime import datetime, timezone
        
        try:
            mem_time = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
            now = datetime.now(timezone.utc)
            delta = now - mem_time
            
            if delta.days > 30:
                return f"{delta.days // 30} months ago"
            elif delta.days > 0:
                return f"{delta.days} days ago"
            elif delta.seconds > 3600:
                return f"{delta.seconds // 3600} hours ago"
            elif delta.seconds > 60:
                return f"{delta.seconds // 60} minutes ago"
            else:
                return "just now"
        except Exception:
            return "recently"

    async def close(self) -> None:
        """Close LLM client."""
        await self.client.close()

