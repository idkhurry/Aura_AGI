# ============================================
# AURA BACKEND CONFIGURATION
# ============================================
# Copy this file to .env and fill in your values

# ============================================
# DATABASE CONFIGURATION (SurrealDB)
# ============================================
SURREAL_URL=ws://localhost:8000/rpc
SURREAL_USER=root
SURREAL_PASS=root
SURREAL_NS=aura
SURREAL_DB=main

# ============================================
# OPENROUTER API CONFIGURATION
# ============================================
# Required for LLM inference and embeddings
# Get your key at: https://openrouter.ai/keys
OPENROUTER_API_KEY=OPENROUTER_API_KEY
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1

# ============================================
# LLM MODEL CONFIGURATION (Hot-Swappable)
# ============================================
# L1 (Instinct Layer): Fast responses, <500ms
# Recommended: Mistral 7B, Gemma, or other lightweight models
AURA_L1_MODEL=mistralai/mistral-7b-instruct

# L2 (Reasoning Layer): Deep analysis, async processing
# Recommended: Claude 3.5 Sonnet, GPT-4, or DeepSeek-R1
AURA_L2_MODEL=anthropic/claude-3.5-sonnet

# L3 (Synthesis Layer): Primary response generation
# Recommended: DeepSeek Chat, Gemini Pro, Claude 3 Haiku
AURA_L3_MODEL=deepseek/deepseek-chat

# L4 (Emotion Analysis Layer): Emotion detection from conversation (async)
# Recommended: Qwen2.5 72B (excellent JSON output, cost-effective), Claude Haiku (fast), or DeepSeek-V3.2 (premium)
# 7B models may struggle with nuanced emotion detection across 27 dimensions
# This layer analyzes conversations to detect emotional content and must return valid JSON
AURA_L4_MODEL=qwen/qwen-2.5-72b-instruct

# L5 (Structure Layer): Specialized JSON extraction, summarization, and structural tasks
# Recommended: Gemini Flash 1.5, Qwen2.5 72B, or other high-reliability JSON models
AURA_L5_MODEL=google/gemini-flash-1.5

# ============================================
# EMBEDDINGS CONFIGURATION (Semantic Search)
# ============================================
# Model for generating vector embeddings (must be 1536 dimensions)
# Recommended: OpenAI text-embedding-3-small (via OpenRouter)
AURA_EMBEDDING_MODEL=openai/text-embedding-3-small
AURA_EMBEDDING_DIMENSION=1536

# ============================================
# APPLICATION CONFIGURATION
# ============================================
ENVIRONMENT=development
LOG_LEVEL=INFO
API_HOST=0.0.0.0
API_PORT=8080
APP_TIMEZONE=Europe/Bucharest
# Common timezones: UTC, Europe/London, Europe/Paris (CET), Europe/Bucharest (EET), US/Eastern, US/Pacific

# ============================================
# EMOTION ENGINE CONFIGURATION
# ============================================
# Seconds between emotion physics ticks
EMOTION_TICK_RATE=5.0

# Seconds between emotion state persistence to database
EMOTION_PERSISTENCE_INTERVAL=60.0

# ============================================
# CORS CONFIGURATION
# ============================================
# Comma-separated list of allowed origins
CORS_ORIGINS=http://localhost:3000,http://localhost:3001

# ============================================
# RECOMMENDED MODEL ALTERNATIVES
# ============================================
# Feel free to experiment! Here are tested alternatives:

# --- Budget-Conscious Setup ---
# AURA_L1_MODEL=google/gemma-7b-it
# AURA_L2_MODEL=anthropic/claude-3-haiku
# AURA_L3_MODEL=google/gemini-flash-1.5
# AURA_L4_MODEL=anthropic/claude-3-haiku  # Fast and cheap, decent JSON

# --- Maximum Performance Setup ---
# AURA_L1_MODEL=mistralai/mistral-7b-instruct
# AURA_L2_MODEL=anthropic/claude-3-opus
# AURA_L3_MODEL=anthropic/claude-3.5-sonnet
# AURA_L4_MODEL=deepseek/deepseek-v3.2  # Premium: best reasoning and JSON ($0.25/$0.38 per M tokens)

# --- Cost-Effective Quality Setup (Recommended) ---
# AURA_L1_MODEL=mistralai/mistral-7b-instruct
# AURA_L2_MODEL=anthropic/claude-3-haiku
# AURA_L3_MODEL=deepseek/deepseek-chat
# AURA_L4_MODEL=qwen/qwen-2.5-72b-instruct  # Excellent JSON output, good price/performance

# --- All-DeepSeek Setup ---
# AURA_L1_MODEL=deepseek/deepseek-chat
# AURA_L2_MODEL=deepseek/deepseek-chat
# AURA_L3_MODEL=deepseek/deepseek-chat
# AURA_L4_MODEL=deepseek/deepseek-v3.2  # Or use deepseek-chat for consistency

# --- Alternative L4 Options ---
# AURA_L4_MODEL=qwen/qwen-2.5-72b-instruct  # Best JSON output, cost-effective (default)
# AURA_L4_MODEL=arliai/qwq-32b-arliai-rpr-v1  # Creative/roleplay focused, good for emotions
# AURA_L4_MODEL=anthropic/claude-3-haiku  # Fast and cheap, decent quality
# AURA_L4_MODEL=deepseek/deepseek-v3.2  # Premium option, best reasoning ($0.25/$0.38 per M)

# --- Experimental Local + Cloud Hybrid ---
# AURA_L1_MODEL=local/phi-3-mini-4k  # If running local Ollama
# AURA_L2_MODEL=anthropic/claude-3.5-sonnet
# AURA_L3_MODEL=deepseek/deepseek-chat
# AURA_L4_MODEL=local/phi-3-mini-4k  # If running local Ollama

# ============================================
# NOTES
# ============================================
# - All model names follow OpenRouter's format: provider/model-name
# - Check available models: https://openrouter.ai/models
# - Pricing varies significantly - monitor your usage!
# - L1 should be fast and cheap (called frequently)
# - L2 can be expensive (runs async in background)
# - L3 should balance speed and quality (primary user interaction)
# - L4 should balance accuracy and cost (runs async after each response for emotion detection)
#   Note: 7B models may struggle with nuanced 27D emotion detection - consider Claude Haiku or GPT-4o-mini

